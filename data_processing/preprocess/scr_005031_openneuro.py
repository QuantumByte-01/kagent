import json,re
from google.cloud import storage
from bs4 import BeautifulSoup

INPUT_GCS_PATH="ks_datasets/raw_dataset/data_sources/scr_005031_openneuro.json"
OUTPUT_GCS_PATH="ks_datasets/preprocessed_data/scr_005031_openneuro.json"
DATASOURCE_ID="scr_005031_openneuro"
DATASOURCE_NAME="OpenNeuro"
DATASOURCE_DESCRIPTION="Public neuroimaging datasets in BIDS format"
DATASOURCE_TYPE="dataset"

def clean_html(h):
    return BeautifulSoup(h or "","html.parser").get_text()
def extract_urls(t):
    return list(set(re.findall(r"https?://[^\s\"<>]+",t or "")))
def safe_join(x,sep="; "):
    if isinstance(x,list):return sep.join(str(v).strip() for v in x if isinstance(v,str) and v.strip())
    return str(x).strip()

client=storage.Client()

b,bl=INPUT_GCS_PATH.split("/",1)
records=json.loads(client.bucket(b).blob(bl).download_as_text())
out=[]

for r in records:
    dc=r.get("dc",{}) or {}
    title=dc.get("title","") or ""
    desc=dc.get("description","") or ""
    ident=dc.get("identifier","") or ""
    lic=r.get("License","") or ""
    authors=r.get("Authors",[]) or []
    ack=r.get("Acknowledgements","") or ""
    how=r.get("HowToAcknowledge","") or ""
    funding=r.get("Funding",[]) or []
    refs=r.get("ReferencesAndLinks",[]) or []
    readme=r.get("readme","") or ""
    bids=r.get("BIDSVersion","") or ""
    dsdoi=r.get("DatasetDOI","") or ""
    rec_id=r.get("id","") or ""
    urls=extract_urls(desc)+extract_urls(readme)
    chunk="\n".join(filter(None,[
        title,
        clean_html(desc),
        safe_join(authors),
        safe_join(funding),
        bids,
        safe_join(refs)
    ]))
    meta={
        "License":lic,
        "authors":authors,
        "Acknowledgements":ack,
        "HowToAcknowledge":how,
        "Funding":funding,
        "ReferencesAndLinks":refs,
        "readme":readme,
        "BIDSVersion":bids,
        "DatasetDOI":dsdoi,
        "id":rec_id,
        "datasource_id":DATASOURCE_ID,
        "datasource_name":DATASOURCE_NAME,
        "datasource_description":DATASOURCE_DESCRIPTION,
        "datasource_type":DATASOURCE_TYPE,
        "identifier":ident
    }
    for i,u in enumerate(urls, start=1):meta[f"identifier{i}"]=u
    out.append({"chunk":chunk,"metadata_filters":meta})
    
print(json.dumps(out[0],ensure_ascii=False,indent=2))

ob,obl=OUTPUT_GCS_PATH.split("/",1)
client.bucket(ob).blob(obl).upload_from_string(json.dumps(out,ensure_ascii=False,indent=2),"application/json")
print(f"Uploaded {len(out)} records to gs://{OUTPUT_GCS_PATH}")

""" 
  "chunk": "Feature Discrimination\nPlease cite the following references if you use these data:\n\nBallard, I.B, Wagner, S.M., McClure, S.M. Hippocampal Pattern Separation Supports Reinforcement Learning. Nature Communications. 2019.\n\nhttps://rdcu.be/bpEQQ\n\nAnalysis code can be found at: https://github.com/iancballard/Hipp_Pattern_Separation_Code\n\nThis dataset is made available under the Public Domain Dedication and License \n\nv1.0, whose full text can be found at \n\nhttp://www.opendatacommons.org/licenses/pddl/1.0/. \n\nWe hope that all users will follow the ODC Attribution/Share-Alike \n\nCommunity Norms (http://www.opendatacommons.org/norms/odc-by-sa/); \n\nin particular, while not legally required, we hope that all users  \n\nof the data will acknowledge the OpenfMRI project and Ian Ballard in any publications.\n\n\nDefacing\n--------\nPydeface was used on all anatomical images to ensure deindentification of subjects. The code\ncan be found at https://github.com/poldracklab/pydeface\n\n\nKnown Issues\n------------\n- CRITICAL: We used a development multiband sequence that has three quirks relevant to data quality:\n\t1) The uppermost slice is corrupted for most subjects, and should be discarded. Take care to do the same for the fieldmap opposite-phase images. \n\t2) On most scans, the last 1-4 volumes are corrupted. It is important to crop these volumes because the high intensity of these images will interfere with preprocessing algorithms. As such, the automated QA reports generated by OpenNeuro will not be accurate.\n\t3) The first 2 volumes of each scan are reconstruction related volumes that have been prepended. The experimental timing has been adjusted so that it lines up with the data (i.e., volume 1 corresponds to time 0 in the events file); however, it would be prudent to remove a larger number of volumes than normal. In the paper, I removed 8 volumes and subtracted 12 seconds from the events timing. \n\t\n- 2 subjects have only 2 out of 3 runs of functional data due to data loss (18 and 23).\n\n- A few subjects have 2 runs of DTI data, the majority do not.\n\n- Sub 15 has faulty reaction time data\n\n- Subject 4 has an anatomical image acquired in a different session with different parameters. \n\n\nIan C. Ballard; Anthony D. Wagner; Samuel M. McClure\nNSF GRFP (ICB), NSF IGERT (ICB) and Stanford Innovation Grants (ICB)\n1.1.1",
  "metadata_filters": {
    "License": "This dataset is made available under the Public Domain Dedication and License \nv1.0, whose full text can be found at \nhttp://www.opendatacommons.org/licenses/pddl/1.0/. \nWe hope that all users will follow the ODC Attribution/Share-Alike \nCommunity Norms (http://www.opendatacommons.org/norms/odc-by-sa/); \nin particular, while not legally required, we hope that all users \nof the data will acknowledge the OpenfMRI project and Ian Ballard in any publications.",
    "authors": [
      "Ian C. Ballard",
      "Anthony D. Wagner",
      "Samuel M. McClure"
    ],
    "Acknowledgements": "",
    "HowToAcknowledge": "Please cite Ballard, I.C, Wagner, A.D., McClure, S.M. Hippocampal Pattern Separation Supports Reinforcement Learning.",
    "Funding": "NSF GRFP (ICB), NSF IGERT (ICB) and Stanford Innovation Grants (ICB)",
    "ReferencesAndLinks": [],
    "readme": "Please cite the following references if you use these data:\n\nBallard, I.B, Wagner, S.M., McClure, S.M. Hippocampal Pattern Separation Supports Reinforcement Learning. Nature Communications. 2019.\n\nhttps://rdcu.be/bpEQQ\n\nAnalysis code can be found at: https://github.com/iancballard/Hipp_Pattern_Separation_Code\n\nThis dataset is made available under the Public Domain Dedication and License \n\nv1.0, whose full text can be found at \n\nhttp://www.opendatacommons.org/licenses/pddl/1.0/. \n\nWe hope that all users will follow the ODC Attribution/Share-Alike \n\nCommunity Norms (http://www.opendatacommons.org/norms/odc-by-sa/); \n\nin particular, while not legally required, we hope that all users  \n\nof the data will acknowledge the OpenfMRI project and Ian Ballard in any publications.\n\n\nDefacing\n--------\nPydeface was used on all anatomical images to ensure deindentification of subjects. The code\ncan be found at https://github.com/poldracklab/pydeface\n\n\nKnown Issues\n------------\n- CRITICAL: We used a development multiband sequence that has three quirks relevant to data quality:\n\t1) The uppermost slice is corrupted for most subjects, and should be discarded. Take care to do the same for the fieldmap opposite-phase images. \n\t2) On most scans, the last 1-4 volumes are corrupted. It is important to crop these volumes because the high intensity of these images will interfere with preprocessing algorithms. As such, the automated QA reports generated by OpenNeuro will not be accurate.\n\t3) The first 2 volumes of each scan are reconstruction related volumes that have been prepended. The experimental timing has been adjusted so that it lines up with the data (i.e., volume 1 corresponds to time 0 in the events file); however, it would be prudent to remove a larger number of volumes than normal. In the paper, I removed 8 volumes and subtracted 12 seconds from the events timing. \n\t\n- 2 subjects have only 2 out of 3 runs of functional data due to data loss (18 and 23).\n\n- A few subjects have 2 runs of DTI data, the majority do not.\n\n- Sub 15 has faulty reaction time data\n\n- Subject 4 has an anatomical image acquired in a different session with different parameters. \n\n",
    "BIDSVersion": "1.1.1",
    "DatasetDOI": "10.18112/openneuro.ds001590.v1.0.1",
    "id": "ds001590",
    "datasource_id": "scr_005031_openneuro",
    "datasource_name": "OpenNeuro",
    "datasource_description": "Public neuroimaging datasets in BIDS format",
    "datasource_type": "dataset",
    "identifier": "https://openneuro.org/datasets/ds001590",
    "identifier1": "http://www.opendatacommons.org/norms/odc-by-sa/);",
    "identifier2": "https://github.com/poldracklab/pydeface",
    "identifier3": "http://www.opendatacommons.org/licenses/pddl/1.0/.",
    "identifier4": "https://github.com/iancballard/Hipp_Pattern_Separation_Code",
    "identifier5": "https://rdcu.be/bpEQQ",
    "identifier6": "http://www.opendatacommons.org/norms/odc-by-sa/);",
    "identifier7": "https://github.com/poldracklab/pydeface",
    "identifier8": "http://www.opendatacommons.org/licenses/pddl/1.0/.",
    "identifier9": "https://github.com/iancballard/Hipp_Pattern_Separation_Code",
    "identifier10": "https://rdcu.be/bpEQQ"
  }
}
Uploaded 1093 records to gs://ks_datasets/preprocessed_data/scr_005031_openneuro.json 
"""